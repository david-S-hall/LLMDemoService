base:
  database_name: &database_name articles
  frontend_route_def: configs/route.yml
  cache_dir: ./cache
  data_dir: ./data

api:
  model:
    host: '0.0.0.0'
    port: 10080
  view:
    host: '0.0.0.0'
    port: 10081
  ui:
    host: '0.0.0.0'
    port: 8501

frontend:
  env:
    # for next-auth
    # NEXTAUTH_URL: 'http://localhost:8501' # for dev
    # NEXTAUTH_SECRET: 'A CHOSEN SECRET TOKEN'

    # GITHUB_ID: "$GITHUB_ID"
    # GITHUB_SECRET: "$GITHUB_SECRET"

    # GOOGLE_ID: "$GOOGLE_ID"
    # GOOGLE_SECRET: "$GOOGLE_SECRET"

    NEXTAUTH_URL: http://localhost:8501
    NEXTAUTH_SECRET: breakingbad

    GITHUB_ID: "85b67cc59137892d35f0"
    GITHUB_SECRET: "7ab00d7518e1c182609e5112448e0863c74988a4"

    GOOGLE_ID: "265503799947-j424h0grim394jk5f9rjrk1fc5gktkda.apps.googleusercontent.com"
    GOOGLE_SECRET: "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"

llm:
  # model_1:
  #   type: ChatGLM
  #   llm_name_or_path: THUDM/chatglm3-6b
  #   multi_gpu_model_cache_dir: ./cache/temp_model_dir
  #   num_gpus: 1
  #   fp16: false # true
  #   quantization_bit: null
  #   ptuning_checkpoint: null
  #   pre_seq_len: null
  #   prefix_projection: false
  # model_2:
  #   type: Qwen
  #   llm_name_or_path: Qwen/Qwen-7B-Chat
  #   device_map: auto
  #   fp16: false
  #   bf16: false
  #   quantization_bit: null
  model_3:
    type: InternLM
    llm_name_or_path: internlm/internlm2-chat-7b
    fp16: False

embeddings:
  model_name: BAAI/bge-base-zh-v1.5
  model_kwargs:
    device: cuda
  encode_kwargs:
    normalize_embeddings: true
  query_instruction: "为这个句子生成表示以用于检索相关文章："

mongo:
  database: chat
  host: 0.0.0.0
  port: 27017
